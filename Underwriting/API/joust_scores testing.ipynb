{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import distance\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "import psycopg2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getscore(jid):\n",
    "    raw = runqueries(jid)\n",
    "    pre = preprocessing(raw)\n",
    "    clean = processing(pre, jid)\n",
    "    norm = normalize(clean)\n",
    "    score = addweights(norm)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runqueries(jid):\n",
    "    try:\n",
    "        with SSHTunnelForwarder(\n",
    "            (os.environ.get('JOUST_IP'), 22),\n",
    "            ssh_private_key=\"~/.ssh/id_rsa\",\n",
    "            ssh_username=\"ubuntu\",\n",
    "            remote_bind_address=(os.environ.get('JOUST_ADDRESS'), 5432)) as server:\n",
    "\n",
    "            server.start()\n",
    "\n",
    "            params = {\n",
    "                \"database\": \"joust_production\",\n",
    "                \"user\": \"sid\",\n",
    "                \"password\": os.environ.get('JOUST_PASS'),\n",
    "                \"host\": \"localhost\",\n",
    "                \"port\": server.local_bind_port\n",
    "            }\n",
    "\n",
    "            conn = psycopg2.connect(**params)\n",
    "            curs = conn.cursor()\n",
    "\n",
    "            #curs.execute(\"select * from public.account_holders\")\n",
    "            transfers = \"select account_holders.id, first_name, last_name, amount, transfers.created_at as \\\"created_at.1\\\", core_pro_customers.customer_id as q2_core_pro_customer_id, from_account_id, transfers.customer_id, to_account_id from account_holders inner join transfers on account_holders.id = transfers.account_holder_id left join core_pro_customers on core_pro_customers.account_holder_id = account_holders.id where account_holders.id = \" + str(jid)\n",
    "            invoices = \"select account_holders.id, first_name, last_name, payment_requests.status, amount, account_holders.created_at, payment_requests.created_at as \\\"created_at.1\\\", accepted_date, core_pro_customers.customer_id as q2_core_pro_customer_id, payarmour_service, payment_type from account_holders inner join payment_requests on account_holders.id = payment_requests.account_holder_id left join core_pro_customers on core_pro_customers.account_holder_id = account_holders.id where account_holders.id = \" + str(jid)\n",
    "            total = \"select account_holders.id, account_holders.segment_intercom_id, first_name, last_name, occupations.\\\"name\\\" as occupation, industries.\\\"name\\\" as industry, marketing.value as legacy_business_url, type_of_work, locations.country, account_holders.created_at, sign_in_count, ssn, email, mobile_phone, date_of_birth, expected_yearly_income_id, core_pro_customers.customer_id as q2_core_pro_customer_id, payarmour_service from account_holders left join core_pro_customers on core_pro_customers.account_holder_id = account_holders.id left join locations on account_holders.id = locations.locatable_id left join occupations on account_holders.occupation_id = occupations.id left join industries on occupations.industry_id = industries.id left join businesses on businesses.account_holder_id = account_holders.id left join marketing on marketing.business_id = businesses.id and marketing.source = 2\"\n",
    "            plaid = \"select account_holders.id, first_name,last_name,plaid_identities.updated_at,plaid_identities.raw_response->>'owners' from account_holders inner join external_bank_accounts on account_holders.id = external_bank_accounts.account_holder_id inner join plaid_identities on external_bank_accounts.id = plaid_identities.plaid_identifiable_id and plaid_identifiable_type='ExternalBankAccount' where account_holders.id = \" + str(jid)\n",
    "            bank = \"select account_holders.id, first_name,last_name,bank_accounts.q2_account_id,bank_accounts.is_primary,bank_accounts.available_balance from account_holders inner join bank_accounts on bank_accounts.account_holder_id = account_holders.id where account_holders.id = \" + str(jid)\n",
    "            devices = \"select account_holders.id, first_name, last_name, devices.signature, devices.platform, devices.created_at  from account_holders inner join account_holder_devices on account_holders.id = account_holder_devices.account_holder_id inner join devices on account_holder_devices.device_id = devices.id\"\n",
    "            stat = \"select account_holders.id, account_holders.created_at, account_holders.archived, account_activations.status, account_holders.email, first_name, last_name, verification_status as q2_core_pro_verification_status from account_holders left join core_pro_customers on account_holders.id = core_pro_customers.account_holder_id left join account_activations on account_holders.id = account_activations.account_holder_id where account_holders.id = \" + str(jid)\n",
    "            events = \"SELECT events.payload -> 'payloadTypeId' AS id, events.payload -> 'data' -> 0 -> 0 -> 'returnCode' AS returncode, events.payload -> 'data' -> 0 -> 0 -> 'type' AS type, events.payload -> 'data' -> 0 -> 0 -> 'amount' AS amount, events.payload -> 'data' -> 0 -> 0 -> 'customerId' AS customerId, events.payload -> 'data' -> 0 -> 0 -> 'createdDate' AS createdDate FROM \\\"events\\\"\"\n",
    "\n",
    "            #result = curs.fetchall()\n",
    "            tra = pd.read_sql_query(transfers, conn)\n",
    "            req = pd.read_sql_query(invoices, conn)\n",
    "            ids = pd.read_sql_query(total, conn)\n",
    "            ids.drop_duplicates(subset =\"id\", inplace = True)\n",
    "            plaid = pd.read_sql_query(plaid, conn)\n",
    "            bank = pd.read_sql_query(bank, conn)\n",
    "            cells = pd.read_sql_query(devices, conn)\n",
    "            rte = pd.read_sql_query(events, conn)\n",
    "            status = pd.read_sql_query(stat, conn)\n",
    "\n",
    "            conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Connection Failed\")\n",
    "        \n",
    "    try:\n",
    "        params = {\n",
    "            \"database\": \"martechdb\",\n",
    "            \"user\": \"sid\",\n",
    "            \"password\": os.environ.get('MARTECH_PASS'),\n",
    "            \"host\": os.environ.get('MARTECH_ADDRESS'),\n",
    "            \"port\": 5439\n",
    "        }\n",
    "\n",
    "        conn = psycopg2.connect(**params)\n",
    "        curs = conn.cursor()\n",
    "\n",
    "        negative = \"select * from ruby.negative_balance_alert\"\n",
    "\n",
    "        negj = pd.read_sql_query(negative, conn)\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Connection Failed\")\n",
    "        \n",
    "    queries = [tra, req, ids, plaid, bank, cells, rte, status, negj]\n",
    "    \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(queries):\n",
    "    tra = queries[0]\n",
    "    req = queries[1]\n",
    "    ids = queries[2]\n",
    "    plaid = queries[3]\n",
    "    bank = queries[4]\n",
    "    cells = queries[5]\n",
    "    rte = queries[6]\n",
    "    status = queries[7]\n",
    "    negj = queries[8]\n",
    "    \n",
    "    ids = ids.rename(columns={\"first_name\": \"First Name\", \"last_name\": \"Last Name\", \"type_of_work\": \"Type Of Work\", \"created_at\": \"Created At\", \"legacy_business_url\": \"business_url\", \"sign_in_count\": \"Sign In Count\"})\n",
    "    req = req.rename(columns={\"first_name\": \"First Name\", \"last_name\": \"Last Name\", \"status\": \"Status\", \"amount\": \"Amount (Payment Requests)\", \"created_at\": \"Created At\", \"created_at.1\": \"Created At (Payment Requests)\", \"accepted_date\": \"Accepted Date\", \"q2_core_pro_customer_id\": \"Q2 Core Pro Customer Id\"})\n",
    "    plaid = plaid.rename(columns={\"first_name\": \"First Name\", \"last_name\": \"Last Name\", \"?column?\": \"Raw Response\"})\n",
    "    tra = tra.rename(columns={\"first_name\": \"First Name\", \"last_name\": \"Last Name\", \"amount\": \"Amount\", \"created_at.1\": \"Created At (Transfers)\", \"q2_core_pro_customer_id\": \"Q2 Core Pro Customer Id\"})\n",
    "    bank = bank.rename(columns={\"first_name\": \"First Name\", \"last_name\": \"Last Name\"})\n",
    "    negj = negj.rename(columns={\"name\": \"Full Name\"})\n",
    "    cells = cells.rename(columns={\"first_name\": \"First Name\", \"last_name\": \"Last Name\"})\n",
    "    \n",
    "    ids = ids[['id','segment_intercom_id','First Name','Last Name','ssn','email','mobile_phone','date_of_birth','business_url','occupation','industry','country','expected_yearly_income_id','q2_core_pro_customer_id','Type Of Work','Created At','payarmour_service','Sign In Count']]\n",
    "    req = req[['id','First Name','Last Name','Status','Amount (Payment Requests)','Created At','Created At (Payment Requests)','Accepted Date','Q2 Core Pro Customer Id','payarmour_service','payment_type']]\n",
    "    tra = tra[['id','First Name','Last Name','Amount','Created At (Transfers)','Q2 Core Pro Customer Id','from_account_id','customer_id','to_account_id']]\n",
    "    negj = negj[['Full Name','balance','user_id','effective_date','type']]\n",
    "    \n",
    "    ids['Created At'] = pd.to_datetime(ids['Created At'])\n",
    "    req['Created At'] = pd.to_datetime(req['Created At'])\n",
    "    req['Created At (Payment Requests)'] = pd.to_datetime(req['Created At (Payment Requests)'])\n",
    "    req['Accepted Date'] = pd.to_datetime(req['Accepted Date'])\n",
    "    tra['Created At (Transfers)'] = pd.to_datetime(tra['Created At (Transfers)'])\n",
    "    negj['effective_date'] = pd.to_datetime(negj['effective_date'])\n",
    "    plaid['updated_at'] =  pd.to_datetime(plaid['updated_at'])\n",
    "    \n",
    "    ids['now'] = datetime.utcnow()\n",
    "    req['now'] = datetime.utcnow()\n",
    "    tra['now'] = datetime.utcnow()\n",
    "    plaid['now'] = datetime.utcnow()\n",
    "    \n",
    "    req['Full Name'] = req['First Name'].astype(str).values + ' ' + req['Last Name'].astype(str).values\n",
    "    req = req.drop(['First Name','Last Name'], axis=1)\n",
    "    tra['Full Name'] = tra['First Name'].astype(str).values + ' ' + tra['Last Name'].astype(str).values\n",
    "    tra = tra.drop(['First Name','Last Name'], axis=1)\n",
    "    plaid['Full Name'] = plaid['First Name'].astype(str).values + ' ' + plaid['Last Name'].astype(str).values\n",
    "    plaid = plaid.drop(['First Name','Last Name'], axis=1)\n",
    "    ids['Full Name'] = ids['First Name'].astype(str).values + ' ' + ids['Last Name'].astype(str).values\n",
    "    ids = ids.drop(['First Name','Last Name'], axis=1)\n",
    "    bank['Full Name'] = bank['First Name'].astype(str).values + ' ' + bank['Last Name'].astype(str).values\n",
    "    bank = bank.drop(['First Name','Last Name'], axis=1)\n",
    "    cells['Full Name'] = cells['First Name'].astype(str).values + ' ' + cells['Last Name'].astype(str).values\n",
    "    cells = cells.drop(['First Name','Last Name'], axis=1)\n",
    "    \n",
    "    tra = tra.merge(bank, how='left',left_on='id', right_on='id')\n",
    "    tra['dir'] = 0\n",
    "    tra['dir'] = (tra['q2_account_id'] == tra['to_account_id'])\n",
    "\n",
    "    rte = rte.dropna()\n",
    "    rte['createddate'] = pd.to_datetime(rte['createddate'])\n",
    "    rte['Year'] = rte['createddate'].dt.year\n",
    "    rte['Month'] = rte['createddate'].dt.month\n",
    "    rte['Day'] = rte['createddate'].dt.day\n",
    "    rte['Date'] = rte[\"Year\"].astype(str) + '-' + rte[\"Month\"].astype(str) + '-' + rte[\"Day\"].astype(str)\n",
    "    rte['Date'] = pd.to_datetime(rte['Date'])\n",
    "    \n",
    "    rcids = rte['customerid'].values\n",
    "    rcodes = rte['returncode'].values\n",
    "    ramounts = rte['amount'].values\n",
    "    rdates = rte['Date'].values\n",
    "    nsfs = pd.DataFrame(rcids, columns=['ids'])\n",
    "    nsfs['code'] = rcodes\n",
    "    nsfs['amount'] = ramounts\n",
    "    nsfs['date'] = rdates\n",
    "    nsfs['date'] = pd.to_datetime(nsfs['date'])\n",
    "    nsfs['now'] = datetime.now()\n",
    "    cptojid = dict(zip(ids['q2_core_pro_customer_id'], ids['id']))\n",
    "    nsfs['ids'] =  nsfs['ids'].replace(cptojid)\n",
    "    \n",
    "    negj['now'] = datetime.now()\n",
    "    negj['time diff'] = (negj['now'] - negj['effective_date']).dt.days\n",
    "    negj = negj[negj['time diff'] < 90]\n",
    "    siidtojid = dict(zip(ids['segment_intercom_id'], ids['id']))\n",
    "    negj['user_id'] =  negj['user_id'].replace(siidtojid)\n",
    "    \n",
    "    ids['timediff'] = (ids['now'] - ids['Created At']).dt.total_seconds() / 3600\n",
    "    req['timediff'] = (req['now'] - req['Created At (Payment Requests)']).dt.total_seconds() / 3600\n",
    "    tra['timediff'] = (tra['now'] - tra['Created At (Transfers)']).dt.total_seconds() / 3600\n",
    "    plaid['timediff'] = (plaid['now'] - plaid['updated_at']).dt.total_seconds() / 3600\n",
    "    nsfs['timediff'] = (nsfs['now'] - nsfs['date']).dt.total_seconds() / 3600\n",
    "\n",
    "    predata = [ids, req, tra, plaid, nsfs, negj, cells]\n",
    "    \n",
    "    return predata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(predata, jid):\n",
    "    cleandata = []\n",
    "    name1 = predata[1].sort_values('Created At (Payment Requests)', ascending=True)\n",
    "    name2 = predata[2].sort_values('Created At (Transfers)', ascending=True)\n",
    "    name3 = predata[3].sort_values('updated_at', ascending=False)\n",
    "    name4 = predata[0][predata[0]['id'] == jid]\n",
    "    name6 = predata[5][predata[5]['user_id'] == jid]\n",
    "    name7 = predata[6][predata[6]['id'] == jid].sort_values('created_at', ascending=False)\n",
    "    atr1 = len(name1[name1['Status'] == 1])\n",
    "    cleandata.append(atr1) \n",
    "    atr1_1 = 0\n",
    "    if (len(name1) > 0):\n",
    "        atr1_1 = len(name1[name1['Status'] == 1])/len(name1)\n",
    "    cleandata.append(atr1_1)\n",
    "    name1['time since start'] = name1['Created At (Payment Requests)'] - name1['Created At']\n",
    "    day1t = name1[name1['time since start'] < timedelta(days=1)]\n",
    "    atr2 = len(day1t)\n",
    "    cleandata.append(atr2)\n",
    "    atr3 = name1['Amount (Payment Requests)'].max()\n",
    "    #cleandata.append(atr3)\n",
    "    maxinv = 2500\n",
    "    atr3_2 = 0\n",
    "    if (len(name1) > 0):\n",
    "        atr3_2 = len(name1[name1['Amount (Payment Requests)'] > (.95*maxinv)])/len(name1)\n",
    "    cleandata.append(atr3_2)\n",
    "    name1mod = name1.copy(deep=True)\n",
    "    name1mod = name1mod[name1mod['Status'] == 1]\n",
    "    name1mod['time diff'] = name1mod['Accepted Date'] - name1mod['Created At (Payment Requests)']\n",
    "    day90 = name1mod[name1mod['time diff'] > timedelta(days=90)]\n",
    "    atr5 = len(day90)\n",
    "    cleandata.append(atr5)\n",
    "    atr6 = len(predata[4][predata[4]['ids'] == jid])\n",
    "    cleandata.append(atr6)\n",
    "    atr7 = -1\n",
    "    if len(name3) < 1:\n",
    "        atr7 = 0\n",
    "    else:\n",
    "        rawres = name3['Raw Response'].values[0]\n",
    "        obj = json.loads(rawres)\n",
    "        bank = obj[0]['names'][0]\n",
    "        real = name4['Full Name'].values[0]\n",
    "        bank = bank.lower()\n",
    "        real = real.lower()\n",
    "        bank2 = bank.split(' ')\n",
    "        if (bank2[-1] == 'llc'):\n",
    "            atr7 = 0\n",
    "        elif (bank2[-1] == 'jr' or bank2[-1] == 'sr' or bank2[-1] == 'jr.' or bank2[-1] == 'sr.'):\n",
    "            bank = bank2[0] + ' ' + bank2[1]\n",
    "            atr7 = distance.levenshtein(bank,real)\n",
    "        else:\n",
    "            bank = bank2[0] + ' ' + bank2[-1]\n",
    "            atr7 = distance.levenshtein(bank,real)\n",
    "    atr7_2 = -1\n",
    "    if len(name3) < 1:\n",
    "        atr7_2 = 0\n",
    "    else:\n",
    "        atr7_2 = 1\n",
    "    cleandata.append(atr7_2)\n",
    "    cleandata.append(atr7)\n",
    "    atr7_3 = atr7 * atr7_2\n",
    "    cleandata.append(atr7_3)\n",
    "    atr8 = len(name6[name6['type'] != 'external'])\n",
    "    cleandata.append(atr8)\n",
    "    starttime = name4['Created At'].values[0]\n",
    "    starttime = datetime.utcfromtimestamp(starttime.astype('O')/1e9)\n",
    "    now = datetime.utcnow()\n",
    "    acctage = (now - starttime).days\n",
    "    atr10 = acctage\n",
    "    cleandata.append(atr10)\n",
    "    maxtra = 1500\n",
    "    atr11 = name2[name2['dir'] == True]['Amount'].max()\n",
    "    cleandata.append(atr11)\n",
    "    atr11_2 = name2[name2['dir'] == False]['Amount'].max()\n",
    "    #cleandata.append(atr11_2)\n",
    "    iden = predata[0][predata[0]['Full Name'] == (predata[0][predata[0]['id'] == jid]['Full Name'].values[0])]\n",
    "    ssnamount = len(iden['ssn'].unique())\n",
    "    mailamount = len(iden['email'].unique())\n",
    "    phoneamount = len(iden['mobile_phone'].unique())\n",
    "    dobamount = len(iden['date_of_birth'].unique())\n",
    "    atr12 = max([ssnamount,mailamount,phoneamount]) - 1\n",
    "    cleandata.append(atr12)\n",
    "    atr13 = 'no info'\n",
    "    if(len(name7) > 0):\n",
    "        atr13 = name7['platform'].values[0]\n",
    "    #cleandata.append(atr13)\n",
    "    atr13_2 = len(name7['signature'].unique())\n",
    "    cleandata.append(atr13_2)\n",
    "    sigs = name7['signature'].unique()\n",
    "    totsigs = 0\n",
    "    for k in sigs:\n",
    "        totsigs = totsigs + len(predata[6][predata[6]['signature'] == k])\n",
    "    atr14 = totsigs - atr13_2\n",
    "    cleandata.append(atr14)\n",
    "    urls = name4.dropna(subset=['business_url'])\n",
    "    loc = name4['country'].unique()\n",
    "    atr15 = (loc[0] == 'US')\n",
    "    cleandata.append(atr15)\n",
    "    atr16 = len(name6[name6['type'] == 'external'])\n",
    "    cleandata.append(atr16)\n",
    "    atr17 = len(urls)>0\n",
    "    cleandata.append(atr17)\n",
    "    atr18 = name4['industry'].values[0]\n",
    "    if pd.isnull(atr18):\n",
    "        atr18 = 'Other'\n",
    "    d = {'Web Mobile & Software Dev':4, 'IT & Networking':4, 'Data Science & Analytics':6, 'Engineering & Architecture':6, 'Design & Creative':3, 'Translation':6, 'Legal':8, 'Admin Support':7, 'Sales & Marketing':4, 'Customer Service':7, 'Accounting & Consulting':8, 'Other':3}\n",
    "    cleandata.append(d[atr18])\n",
    "    atr19 = name4['occupation'].values[0]\n",
    "    #cleandata.append(atr19)\n",
    "    name = name4['Full Name'].values[0]\n",
    "    segid = name4['segment_intercom_id'].values[0]\n",
    "    caps = sum(1 for c in name if c.isupper())\n",
    "    atr20 = 1\n",
    "    if (len(name.replace(\" \", \"\")) != 0):\n",
    "        atr20 = caps/len(name.replace(\" \", \"\"))\n",
    "    cleandata.append(atr20)\n",
    "    atr22 = name4['expected_yearly_income_id'].values[0]\n",
    "    cleandata.append(atr22)\n",
    "    tid = jid\n",
    "    cleandata.append(tid)\n",
    "    cleandata = np.nan_to_num(cleandata)\n",
    "    cleandata = np.multiply(cleandata, 1) \n",
    "    cleandata = np.append(cleandata, segid)\n",
    "    \n",
    "    return cleandata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(cleandata):\n",
    "    #here is where we can read/choose which values to include\n",
    "    #here is where we can read/choose the exponents\n",
    "    cleandata3 = cleandata\n",
    "    cleandata2 = [float(i) for i in cleandata[:-1]]\n",
    "    cleandata = cleandata2\n",
    "    normdata = []\n",
    "    Successful_Invoices = cleandata[0]\n",
    "    Successful_Invoices = expnorm(Successful_Invoices, 0, 6, .1)\n",
    "    normdata.append(Successful_Invoices)\n",
    "    Invoice_Pct = cleandata[1]\n",
    "    Invoice_Pct = expnorm(Invoice_Pct, 0, .8, .25)\n",
    "    normdata.append(Invoice_Pct)\n",
    "    First_24_Hour = cleandata[2]\n",
    "    First_24_Hour = expnorm(First_24_Hour, 3, 1, .5)\n",
    "    normdata.append(First_24_Hour)\n",
    "    Big_Invoice_Pct = cleandata[3]\n",
    "    Big_Invoice_Pct = expnorm(Big_Invoice_Pct, 1, 0, .25)\n",
    "    normdata.append(Big_Invoice_Pct)\n",
    "    Overdue_Invoices = cleandata[4]\n",
    "    Overdue_Invoices = expnorm(Overdue_Invoices, 4, 0, 1.25)\n",
    "    normdata.append(Overdue_Invoices)\n",
    "    NSF_count = cleandata[5]\n",
    "    NSF_count = expnorm(NSF_count, 3, 0, 1.75)\n",
    "    normdata.append(NSF_count)\n",
    "    Linked_Acct = cleandata[6]\n",
    "    Linked_Acct = expnorm(Linked_Acct, 0, 1, 1)\n",
    "    normdata.append(Linked_Acct)\n",
    "    Linked_Name = cleandata[7]\n",
    "    Linked_Name = expnorm(Linked_Name, 0, 1, 1.75)\n",
    "    normdata.append(Linked_Name)\n",
    "    Linked_Transfer = cleandata[8]\n",
    "    Linked_Transfer = expnorm(Linked_Transfer, 0, 2, 2)\n",
    "    normdata.append(Linked_Transfer)\n",
    "    Negative_Joust = cleandata[9]\n",
    "    Negative_Joust = expnorm(Negative_Joust, 1, 0, 2)\n",
    "    normdata.append(Negative_Joust)\n",
    "    Acct_Age = cleandata[10]\n",
    "    Acct_Age = expnorm(Acct_Age, 0, 60, .75)\n",
    "    normdata.append(Acct_Age)\n",
    "    Biggest_Deposit = cleandata[11]\n",
    "    Biggest_Deposit = expnorm(Biggest_Deposit, 1500, 1425, 1)\n",
    "    normdata.append(Biggest_Deposit)\n",
    "    Attempts = cleandata[12]\n",
    "    Attempts = expnorm(Attempts, 2, 0, 1.25)\n",
    "    normdata.append(Attempts)\n",
    "    Mult_Phones = cleandata[13]\n",
    "    Mult_Phones = expnorm(Mult_Phones, 2, 0, 1.5)\n",
    "    normdata.append(Mult_Phones)\n",
    "    Repeat_Phone = cleandata[14]\n",
    "    Repeat_Phone = expnorm(Repeat_Phone, 1, 0, 1)\n",
    "    normdata.append(Repeat_Phone)\n",
    "    Domestic = cleandata[15]\n",
    "    Domestic = expnorm(Domestic, 0, 1, 2)\n",
    "    normdata.append(Domestic)\n",
    "    Negative_External = cleandata[16]\n",
    "    Negative_External = expnorm(Negative_External, 2, 0, .2)\n",
    "    normdata.append(Negative_External)\n",
    "    Website = cleandata[17]\n",
    "    Website = expnorm(Website, 0, 1, 1)\n",
    "    normdata.append(Website)\n",
    "    Industry = cleandata[18]\n",
    "    Industry = expnorm(Industry, 0, 10, .5)\n",
    "    normdata.append(Industry)\n",
    "    Capitals = cleandata[19]\n",
    "    Capitals = expnorm(Capitals, 1, 0, .75)\n",
    "    normdata.append(Capitals)\n",
    "    Income = cleandata[20]\n",
    "    Income = expnorm(Income, 0, 5, .1)\n",
    "    normdata.append(Income)\n",
    "    jid = cleandata[21]\n",
    "    normdata.append(jid)\n",
    "    segment = cleandata3[22]\n",
    "    normdata.append(segment)\n",
    "    \n",
    "    return normdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expnorm(val, mini, maxi, exponent):\n",
    "    if val > max(mini,maxi):\n",
    "        val = maxi\n",
    "    if val < min(mini,maxi):\n",
    "        val = mini\n",
    "    expval = 100 * (((val - mini)/(maxi - mini))**exponent)\n",
    "    \n",
    "    return expval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addweights(normdata):\n",
    "    #here is where we can read/choose the weights\n",
    "    setweights = [75, 100, 25, 50, 75, 100, 50, 100, 100, 100, 75, 75, 50, 75, 100, 100, 100, 25, 75, 100, 75]\n",
    "    setweights = np.array(setweights) / sum(setweights)\n",
    "    score = 0\n",
    "    for k in range(len(setweights)):\n",
    "        score = score + (setweights[k]*normdata[k])\n",
    "        \n",
    "    return [score,normdata[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = getscore(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63.734369541773184, '464ef69a-1cbc-4da0-a489-899442848692']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
